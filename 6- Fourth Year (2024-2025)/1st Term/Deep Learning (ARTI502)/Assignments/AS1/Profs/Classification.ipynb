{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1: Iris Data Classification (Using TensorFlow)\n",
    "## Prepared by [Mustafa Youldash, Ph.D.](https://github.com/youldash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name:\n",
    "### Student ID:\n",
    "### Section Number:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris Data Set (i.e., Problem Set)\n",
    "\n",
    "The [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris/) is a popular data set for classification tasks in machine learning. It consists of 150 samples of iris plants, with each sample consisting of four features (sepal length, sepal width, petal length, and petal width) and a target label indicating the species of the iris plant (setosa, versicolor, or virginica)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the assignment using the Iris data set, students would need to preprocess the data, develop and train a Deep Learning model, and evaluate the performance of the model. Preprocessing the data might involve scaling the features and splitting the data into training and validation sets. Developing and training the model could involve selecting an appropriate architecture and optimization algorithm, setting the learning rate, and choosing the number of epochs. Evaluating the performance of the model could involve using metrics such as accuracy, precision, and recall to assess the model's ability to classify the iris plants correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you currently have?\n",
    "import sys\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Do you have TensorFlow installed on your system?\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Do you have TensorFlow installed on your system?\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Functions for Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import helper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a process of analyzing and summarizing a data set in order to understand the underlying structure and relationships within the data. EDA is an important step in the data science process, as it allows you to identify patterns, trends, and anomalies in the data that may not be immediately apparent.\n",
    "\n",
    "There are several benefits of performing EDA for Deep Learning:\n",
    "\n",
    "- By performing EDA, you can get a better understanding of the data you are working with, including the distribution of the data, the relationships between different features, and any missing or corrupted values.\n",
    "- EDA can help you identify potential problems with the data, such as missing values or outliers, which could impact the performance of your Deep Learning model.\n",
    "- EDA can help you understand the characteristics of the data, which can inform your choice of Deep Learning model. For example, if the data is highly non-linear, you may want to consider using a model that is capable of capturing complex relationships, such as a neural network.\n",
    "- By understanding the underlying structure of the data, you can better tune the hyperparameters of your Deep Learning model, which can lead to improved performance.\n",
    "\n",
    "In the end, EDA is an important step in the Deep Learning process, as it helps you understand the data and identify potential issues that could impact the performance of your model. EDA is open-ended, and it is up to you to decide how to look at different ways to slice and dice your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path, \"iris.csv\")    \n",
    "df = pd.read_csv(filename, na_values=['NA','?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: use a DataFrame for both EDA and model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Flower Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path.\n",
    "path = \"./data/\"\n",
    "\n",
    "# Read the data.\n",
    "filename = os.path.join(path, \"iris.csv\")    \n",
    "df = pd.read_csv(filename, na_values=['NA','?'])\n",
    "\n",
    "# Encode text values to indexes (i.e., [1],[2],[3] for (red,green,blue) values).\n",
    "species = helper.encode_text_index(df, \"species\")\n",
    "\n",
    "# Convert a Pandas DataFrame to the (x,y) inputs that TensorFlow needs.\n",
    "x, y = helper.to_xy(df, \"species\")\n",
    "\n",
    "# Split the data into training and testing sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=FIXME, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, and build your model.\n",
    "model = Sequential()\n",
    "model.add(Dense(FIXME, input_dim=x.shape[1], kernel_initializer='normal',\n",
    "                activation='FIXME')) # Hint: try different activation functions and see which one produces better results.\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "model.add(Dense(y.shape[1],activation='FIXME'))\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "    loss='FIXME',\n",
    "    optimizer='FIXME') # Hint: try different optimizers and see which one produces better results.\n",
    "\n",
    "# Define the training callbacks.\n",
    "monitor = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "    x, y, validation_data=(x_test,y_test),\n",
    "    callbacks=[monitor], verbose=2, epochs=FIXME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# Evaluate the success rate using accuracy.\n",
    "pred = FIXME\n",
    "\n",
    "y_compare = FIXME\n",
    "\n",
    "# Log the accuracy score.\n",
    "score = metrics.accuracy_score(y_compare, pred)\n",
    "print(\"Accuracy score: {}\".format(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
